{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8vLW32R-Yzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d65d1b-c77d-4612-9f41-0814ece57633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# Load a pre-trained VGG-16 model\n",
        "vgg16 = models.vgg16(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgr7ppyoDlMn",
        "outputId": "29d381c5-6c4c-4216-d097-d36867757dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 73.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RPN(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(RPN, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.cls_layer = nn.Conv2d(512, 18, kernel_size=1, stride=1)  # 9 anchors * 2 (object/not object)\n",
        "        self.reg_layer = nn.Conv2d(512, 36, kernel_size=1, stride=1)  # 9 anchors * 4 (coordinates)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv(x))\n",
        "        cls_output = self.cls_layer(x)  # Objectness scores\n",
        "        reg_output = self.reg_layer(x)  # Bounding box coordinates\n",
        "        return cls_output, reg_output"
      ],
      "metadata": {
        "id": "vYEw0hu8DxgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RPN is a small network that predicts region proposals.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kvhe2MESENjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fast R-CNN takes the region proposals from the RPN and classifies them.\n",
        "\n"
      ],
      "metadata": {
        "id": "4PpQtG_2EhhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FastRCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FastRCNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(512 * 7 * 7, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.cls_layer = nn.Linear(4096, num_classes)\n",
        "        self.reg_layer = nn.Linear(4096, num_classes * 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # this to flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        cls_output = self.cls_layer(x)\n",
        "        reg_output = self.reg_layer(x)\n",
        "        return cls_output, reg_output"
      ],
      "metadata": {
        "id": "7mgGNS6QEqTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s combine the RPN and Fast R-CNN into a single Faster R-CNN model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mymt9jc1E61R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FasterRCNN(nn.Module):\n",
        "    def __init__(self, backbone, num_classes):\n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.rpn = RPN(in_channels=512)\n",
        "        self.fast_rcnn = FastRCNN(num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        cls_scores, reg_coords = self.rpn(features)\n",
        "        # For Apply RoI Pooling\n",
        "        rois = self._generate_rois(reg_coords)\n",
        "        cls_output, reg_output = self.fast_rcnn(rois)\n",
        "        return cls_output, reg_output\n",
        "\n",
        "    def _generate_rois(self, reg_coords):\n",
        "        # This is like Simplified RoI generation (but not actual RoI pooling)\n",
        "        return reg_coords"
      ],
      "metadata": {
        "id": "QMa4pB-NE1vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "llmClskmFJJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training our model with an example"
      ],
      "metadata": {
        "id": "4d9Y5zUKGZJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "import os\n"
      ],
      "metadata": {
        "id": "NRJQuPbAHKk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms for the dataset\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(ToTensor())  # Convert PIL image to tensor\n",
        "    if train:\n",
        "        transforms.append(Resize((600, 600)))  # Resize images for training\n",
        "    return Compose(transforms)\n",
        "\n",
        "# Load the PASCAL VOC dataset\n",
        "def get_voc_dataset(root, year=\"2012\", image_set=\"train\", download=True):\n",
        "    # Define the path to the dataset\n",
        "    dataset_path = os.path.join(root, f\"VOC{year}\")\n",
        "\n",
        "    # Load the dataset\n",
        "    dataset = VOCDetection(\n",
        "        root=dataset_path,\n",
        "        year=year,\n",
        "        image_set=image_set,\n",
        "        download=download,\n",
        "        transforms=get_transform(train=(image_set == \"train\"))\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# Example usage\n",
        "root = \"./data\"  # Directory where the dataset will be stored\n",
        "train_dataset = get_voc_dataset(root, year=\"2012\", image_set=\"train\", download=True)\n",
        "val_dataset = get_voc_dataset(root, year=\"2012\", image_set=\"val\", download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExdHk1uILBRv",
        "outputId": "caddacf8-39b9-4b8d-d105-d1d31299883d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ./data/VOC2012/VOCtrainval_11-May-2012.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.00G/2.00G [00:51<00:00, 39.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/VOC2012/VOCtrainval_11-May-2012.tar to ./data/VOC2012\n",
            "Using downloaded and verified file: ./data/VOC2012/VOCtrainval_11-May-2012.tar\n",
            "Extracting ./data/VOC2012/VOCtrainval_11-May-2012.tar to ./data/VOC2012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for object detection datasets.\n",
        "    Args:\n",
        "        batch: A list of tuples (image, target) from the dataset.\n",
        "    Returns:\n",
        "        images: A tensor of shape [batch_size, C, H, W].\n",
        "        targets: A list of dictionaries containing \"boxes\" and \"labels\".\n",
        "    \"\"\"\n",
        "    images = [item[0] for item in batch]  # Extract images\n",
        "    targets = [item[1] for item in batch]  # Extract targets\n",
        "\n",
        "    # Stack images into a single tensor\n",
        "    images = torch.stack(images, dim=0)\n",
        "\n",
        "    return images, targets"
      ],
      "metadata": {
        "id": "3MBteA__MUY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pUBi2NInMapP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader for training and validation\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "gzo5dqNXMEyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Define transforms\n",
        "def get_transform(train):\n",
        "    \"\"\"\n",
        "    Apply transforms to the image only (not the target).\n",
        "    \"\"\"\n",
        "    transforms = []\n",
        "    transforms.append(ToTensor())  # Convert PIL image to tensor\n",
        "    if train:\n",
        "        transforms.append(Resize((600, 600)))  # Resize images for training\n",
        "    return Compose(transforms)\n",
        "\n",
        "# Parse XML annotations\n",
        "def parse_voc_xml(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for obj in root.findall(\"object\"):\n",
        "        # Get class label\n",
        "        label = obj.find(\"name\").text\n",
        "        labels.append(label)\n",
        "\n",
        "        # Get bounding box coordinates\n",
        "        bbox = obj.find(\"bndbox\")\n",
        "        xmin = float(bbox.find(\"xmin\").text)\n",
        "        ymin = float(bbox.find(\"ymin\").text)\n",
        "        xmax = float(bbox.find(\"xmax\").text)\n",
        "        ymax = float(bbox.find(\"ymax\").text)\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    return boxes, labels\n",
        "\n",
        "# Custom dataset class\n",
        "class VOCDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, year=\"2012\", image_set=\"train\", transforms=None):\n",
        "        self.voc = VOCDetection(root, year=year, image_set=image_set, download=True)\n",
        "        self.transforms = transforms\n",
        "        self.classes = [\n",
        "            \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
        "            \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "            \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.voc)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and target\n",
        "        img, target = self.voc[idx]\n",
        "\n",
        "        # Parse XML annotations\n",
        "        boxes, labels = parse_voc_xml(target[\"annotation\"])\n",
        "\n",
        "        # Convert boxes and labels to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor([self.classes.index(label) for label in labels], dtype=torch.int64)\n",
        "\n",
        "        # Create target dictionary\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        # Apply transforms to the image only\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    images = torch.stack(images, dim=0)\n",
        "    return images, targets\n",
        "\n",
        "# Load datasets\n",
        "root = \"./data\"\n",
        "train_dataset = VOCDataset(root, year=\"2012\", image_set=\"train\", transforms=get_transform(train=True))\n",
        "val_dataset = VOCDataset(root, year=\"2012\", image_set=\"val\", transforms=get_transform(train=False))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Define Faster R-CNN model\n",
        "backbone = torchvision.models.vgg16(pretrained=True).features\n",
        "backbone.out_channels = 512\n",
        "anchor_generator = AnchorGenerator(\n",
        "    sizes=((32, 64, 128, 256, 512),),\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
        ")\n",
        "\n",
        "model = FasterRCNN(\n",
        "    backbone,\n",
        "    num_classes=21,  # 20 classes + background\n",
        "    rpn_anchor_generator=anchor_generator\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for images, targets in train_dataloader:\n",
        "        # Move images and targets to the device (GPU or CPU)\n",
        "        images = images.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for logging\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_dataloader):.4f}\")\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, targets in val_dataloader:\n",
        "        # Move images and targets to the device\n",
        "        images = images.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Accumulate loss for logging\n",
        "        val_loss += losses.item()\n",
        "\n",
        "# Print the average validation loss\n",
        "print(f\"Validation Loss: {val_loss/len(val_dataloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "6a6B8PcVMk58",
        "outputId": "6138937d-b949-4b32-e1dd-acc0b29f7664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ./data/VOCtrainval_11-May-2012.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.00G/2.00G [03:11<00:00, 10.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n",
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-26-b634fc37801e>\", line 64, in __getitem__\n    boxes, labels = parse_voc_xml(target[\"annotation\"])\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-26-b634fc37801e>\", line 25, in parse_voc_xml\n    tree = ET.parse(xml_file)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/xml/etree/ElementTree.py\", line 1219, in parse\n    tree.parse(source, parser)\n  File \"/usr/lib/python3.11/xml/etree/ElementTree.py\", line 570, in parse\n    source = open(source, \"rb\")\n             ^^^^^^^^^^^^^^^^^^\nTypeError: expected str, bytes or os.PathLike object, not dict\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b634fc37801e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Move images and targets to the device (GPU or CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"<ipython-input-26-b634fc37801e>\", line 64, in __getitem__\n    boxes, labels = parse_voc_xml(target[\"annotation\"])\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-26-b634fc37801e>\", line 25, in parse_voc_xml\n    tree = ET.parse(xml_file)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/xml/etree/ElementTree.py\", line 1219, in parse\n    tree.parse(source, parser)\n  File \"/usr/lib/python3.11/xml/etree/ElementTree.py\", line 570, in parse\n    source = open(source, \"rb\")\n             ^^^^^^^^^^^^^^^^^^\nTypeError: expected str, bytes or os.PathLike object, not dict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define transforms\n",
        "def get_transform(train):\n",
        "    \"\"\"\n",
        "    Apply transforms to the image only (not the target).\n",
        "    \"\"\"\n",
        "    transforms = []\n",
        "    transforms.append(ToTensor())  # Convert PIL image to tensor\n",
        "    if train:\n",
        "        transforms.append(Resize((600, 600)))  # Resize images for training\n",
        "    return Compose(transforms)\n",
        "\n",
        "# Extract bounding boxes and labels from the target dictionary\n",
        "def extract_boxes_and_labels(target):\n",
        "    \"\"\"\n",
        "    Extract bounding boxes and labels from the VOC target dictionary.\n",
        "    \"\"\"\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for obj in target[\"annotation\"][\"object\"]:\n",
        "        # Get class label\n",
        "        label = obj[\"name\"]\n",
        "        labels.append(label)\n",
        "\n",
        "        # Get bounding box coordinates\n",
        "        bbox = obj[\"bndbox\"]\n",
        "        xmin = float(bbox[\"xmin\"])\n",
        "        ymin = float(bbox[\"ymin\"])\n",
        "        xmax = float(bbox[\"xmax\"])\n",
        "        ymax = float(bbox[\"ymax\"])\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "    return boxes, labels\n",
        "\n",
        "# Custom dataset class\n",
        "class VOCDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, year=\"2012\", image_set=\"train\", transforms=None):\n",
        "        self.voc = VOCDetection(root, year=year, image_set=image_set, download=True)\n",
        "        self.transforms = transforms\n",
        "        self.classes = [\n",
        "            \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
        "            \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "            \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.voc)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and target\n",
        "        img, target = self.voc[idx]\n",
        "\n",
        "        # Extract bounding boxes and labels from the target dictionary\n",
        "        boxes, labels = extract_boxes_and_labels(target)\n",
        "\n",
        "        # Convert boxes and labels to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor([self.classes.index(label) for label in labels], dtype=torch.int64)\n",
        "\n",
        "        # Create target dictionary\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        # Apply transforms to the image only\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    images = torch.stack(images, dim=0)\n",
        "    return images, targets\n",
        "\n",
        "# Load datasets\n",
        "root = \"./data\"\n",
        "train_dataset = VOCDataset(root, year=\"2012\", image_set=\"train\", transforms=get_transform(train=True))\n",
        "val_dataset = VOCDataset(root, year=\"2012\", image_set=\"val\", transforms=get_transform(train=False))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Define Faster R-CNN model\n",
        "backbone = torchvision.models.vgg16(pretrained=True).features\n",
        "backbone.out_channels = 512\n",
        "anchor_generator = AnchorGenerator(\n",
        "    sizes=((32, 64, 128, 256, 512),),\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
        ")\n",
        "\n",
        "model = FasterRCNN(\n",
        "    backbone,\n",
        "    num_classes=21,  # 20 classes + background\n",
        "    rpn_anchor_generator=anchor_generator\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for images, targets in train_dataloader:\n",
        "        # Move images and targets to the device (GPU or CPU)\n",
        "        images = images.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for logging\n",
        "        epoch_loss += losses.item()\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_dataloader):.4f}\")\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, targets in val_dataloader:\n",
        "        # Move images and targets to the device\n",
        "        images = images.to(device)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Accumulate loss for logging\n",
        "        val_loss += losses.item()\n",
        "\n",
        "# Print the average validation loss\n",
        "print(f\"Validation Loss: {val_loss/len(val_dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "SFuLNoXxU_IF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}